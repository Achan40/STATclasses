)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1.1)/2),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/2, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/2),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/2),
MULTI = multinom(classes ~ ., data = sim_est)
)
predict(NaviB_mod, predicty, type = "raw")
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
c(1,1,1)/3
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
test_pred = map(modlist, predict, sim_val)#make predictions for each model using tst data
map_dbl(test_pred,calc_misclass, sim_val$classes)
library("caret")
# set seed
set.seed(77350)
# simulate dataset
sim_est = as_tibble(mlbench.2dnormals(n = 0100, cl = 4, sd = 1.5))
sim_val = as_tibble(mlbench.2dnormals(n = 1000, cl = 4, sd = 1.5))
# check data
sim_est
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
test_pred = map(modlist, predict, sim_val)#make predictions for each model using tst data
map_dbl(test_pred,calc_misclass, sim_val$classes)
#7.8
# load packages
library("MASS")
library("nnet")
library("klaR")
library("mlbench")
library("tibble")
library("caret")
library("tidyverse")
# set seed
set.seed(77350)
# simulate dataset
sim_est = as_tibble(mlbench.2dnormals(n = 0100, cl = 4, sd = 1.5))
sim_val = as_tibble(mlbench.2dnormals(n = 1000, cl = 4, sd = 1.5))
# check data
sim_est
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
test_pred = map(modlist, predict, sim_val)#make predictions for each model using tst data
map_dbl(test_pred,calc_misclass, sim_val$classes)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1)/),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1)/2),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1)),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)),
MULTI = multinom(classes ~ ., data = sim_est)
)
#7.8
# load packages
library("MASS")
library("nnet")
library("klaR")
library("mlbench")
library("tibble")
library("caret")
library("tidyverse")
# set seed
set.seed(77350)
# simulate dataset
sim_est = as_tibble(mlbench.2dnormals(n = 0100, cl = 4, sd = 1.5))
sim_val = as_tibble(mlbench.2dnormals(n = 1000, cl = 4, sd = 1.5))
# check data
sim_est
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
?lda
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior,
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
= c(1,1,1)/3
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1)/3, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1)/3),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1)/3),
MULTI = multinom(classes ~ ., data = sim_est)
)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
test_pred = map(modlist, predict, sim_val)#make predictions for each model using tst data
map_dbl(test_pred,calc_misclass, sim_val$classes)
#calculation for misclassification [est to val]
test_pred = map(modlist, predict, sim_val)#make predictions for each model using tst data
map_dbl(test_pred,calc_misclass$class, sim_val$classes)
test_pred
calc_misclass(act = sim_val$classes,test_pred[1])
#calculation for misclassification [est to val]
test_pred = map(modlist, predict, sim_est)#make predictions for each model using tst data
map_dbl(test_pred,calc_misclass$class, sim_est$classes)
#calculation for misclassification [est to val]
calc_misclass(sim_est$classes, predict(modlist[1], data=sim_est,type = "class"))
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[1], data=sim_val,type = "class"))
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[1], data=sim_val,type = "raw"))
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[1], data=sim_val,type = "raw"))
#7.8
# load packages
library("MASS")
library("nnet")
library("klaR")
library("mlbench")
library("tibble")
library("caret")
library("tidyverse")
# set seed
set.seed(77350)
# simulate dataset
sim_est = as_tibble(mlbench.2dnormals(n = 0100, cl = 4, sd = 1.5))
sim_val = as_tibble(mlbench.2dnormals(n = 1000, cl = 4, sd = 1.5))
# check data
sim_est
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
calc_misclass(sim_est$classes, predict(modlist[1], data=sim_est,type = "raw"))
#7.8
# load packages
library("MASS")
library("nnet")
library("klaR")
library("mlbench")
library("tibble")
library("caret")
library("tidyverse")
# set seed
set.seed(77350)
# simulate dataset
sim_est = as_tibble(mlbench.2dnormals(n = 0100, cl = 4, sd = 1.5))
sim_val = as_tibble(mlbench.2dnormals(n = 1000, cl = 4, sd = 1.5))
# check data
sim_est
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[1], data=sim_val,type = "raw"))
modlist[1]
NavieB = NaiveBayes(classes ~., data = sim_est)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(NavieB, data=sim_val,type = "raw"))
#calculation for misclassification [est to val]
calc_misclass(sim_est$classes, predict(NavieB, data=sim_est,type = "raw"))
#calculation for misclassification [est to val]
calc_misclass(sim_est$classes, predict(NavieB, data=sim_est))
#calculation for misclassification [est to val]
calc_misclass(sim_est$classes, predict(modlist[1], data=sim_est,type = "raw"))
calc_misclass(predict(mod_list[[3]], sim_val)$class, sim_val$classes)
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
calc_misclass(sim_est$classes, predict(modlist[1], data=sim_est,type = "raw"))
calc_misclass(predict(mod_list[[3]], sim_val)$class, sim_val$classes)
calc_misclass(predict(modlist[[3]], sim_val)$class, sim_val$classes)
calc_misclass(predict(modlist[[1]], sim_val)$class, sim_val$classes)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[[1]], sim_val)$class)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[[1]], sim_val)$classes)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[[1]], sim_val)$classes)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[[1]], sim_val)$class)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[[1]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[2]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[3]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[4]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[5]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[6]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[7]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[8]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[8]], sim_val))
# set seed
set.seed(66522)
# simulate dataset
sim_est = as_tibble(mlbench.2dnormals(n = 0100, cl = 4, sd = 1.5))
sim_val = as_tibble(mlbench.2dnormals(n = 1000, cl = 4, sd = 1.5))
# check data
sim_est
# helper function for misclassification rate
calc_misclass = function(act, pred) {
mean(act != pred)
}
modlist = list(
NavieB = NaiveBayes(classes ~., data = sim_est),
NavieB_Flat = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
NavieB_Flat_KDE = NaiveBayes(classes ~., data = sim_est, prior = c(1,1,1,1)/4, usekernel = T),
LDA = lda(classes ~., data = sim_est),
LDA_Flat = lda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
QDA = qda(classes ~., data = sim_est),
QDA_Flat = qda(classes ~., data = sim_est, prior = c(1,1,1,1)/4),
MULTI = multinom(classes ~ ., data = sim_est)
)
#calculation for misclassification [est to val]
calc_misclass(sim_val$classes, predict(modlist[[1]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[2]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[3]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[4]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[5]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[6]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[7]], sim_val)$class)
calc_misclass(sim_val$classes, predict(modlist[[8]], sim_val))
predict(mlmod2,Resort[-train,])
library(faraway)
library(randomForest)
library(car)
setwd("/Users/chanm/Desktop/STATclasses/Aaron/STAT425/Final")
Resort = data.frame(read.csv(file = "stat425_fpdata.csv",header = T))
#Select only the data for Section 1
Resort = Resort[Resort$hotel == "Resort Hotel",]
#Exploratory Analysis
#Remove unneccsary time variables as well as uneccesarry variables
Resort = subset(Resort, select = -c(arrival_date_year, arrival_date_week_number, arrival_date_day_of_month, hotel, reserved_room_type, meal, market_segment, customer_type))
Resort$is_canceled = as.factor(Resort$is_canceled)
#Testing Non-linearity
#Figure 1
par(mfrow = c(2,2))
plot(Resort$lead_time,Resort$adr, ylab = "adr", xlab = "lead_time")
plot(Resort$stays_in_weekend_nights, Resort$adr, ylab = "adr", xlab = "stays_in_weekend_nights")
plot(Resort$stays_in_week_nights, Resort$adr, ylab = "adr", xlab = "stays_in_week_nights")
plot(Resort$total_of_special_requests, Resort$adr, ylab = "adr", xlab = "total_of_special_requests")
title("Figure 1", line = -1, outer = T)
#Model creation
#Simple model with interation term (base model)
mod1 = glm(adr ~ ., data = Resort)
plot(mod1)
summary(mod1)
#Residuals vs Fitted shows that we can assume a linear relationship between explanatory variables and the response
#Normal QQ plot shows our residuals are not exactly normal, perform shapiro test to be sure
shapiro.test(residuals(mod1))
#The result of sharpio-wilks testing H0 = data is normally distributed vs Ha = data is not normally distributed, shows we reject H0, data is not normally distributed. May have to perform transformation of our data
#Scale-Location plot shows variance is not homogenous, may need to perform a transformation
#Residuals vs Leverage plot shows that we have some high leverage points, some exceed 3 standard deviations, we may have to remove them top get a better model, could be a result of non-normality in our data
#influential points
plot(mod1,4)
#collinearity
vif(mod1)
#Model using backward regression w/ AIC and with interaction terms
mod2 = glm(adr ~ . - children - babies + adults:children + adults:babies,data = Resort)
mod.back = step(mod2, direction = "backward")
summary(mod.back)
plot(mod.back)
#Residuals vs Fitted shows that we can assume a linear relationship between explanatory variables and the response
#Normal QQ plot shows our residuals are not exactly normal, perform shapiro test to be sure
shapiro.test(residuals(mod.back))
#The result of sharpio-wilks testing H0 = data is normally distributed vs Ha = data is not normally distributed, shows we reject H0, data is not normally distributed. May have to perform transformation of our data
#Scale-Location plot shows variance is not homogenous, may need to perform a transformation
#Residuals vs Leverage plot shows that we have some high leverage points, some exceed 3 standard deviations, we may have to remove them top get a better model, could be a result of non-normality in our data
#influential points
plot(mod.back,4)
#collinearity
vif(mod2)
r2mod = regsubsets(adr ~ . - children - babies + adults:children + adults:babies,data = Resort)
r2s = summary(r2mod)
#Adjusted R-square to determine significant variables
library(leaps)
r2s$which
msize = 2:9
r2s$which[which.max(r2s$adjr2),]
selectVar = colnames(r2s$which)[r2s$which[which.max(r2s$adjr2),]]
selectVar = selectVar[-1]
selectVar
#Mallow's CP to determine significant variables
r2s$which[which.min(r2s$cp),]
selectVar = colnames(r2s$which)[r2s$which[which.min(r2s$cp),]]
selectVar = selectVar[-1]
selectVar
#Split into train and validation data
set.seed(100)
#collinearity
vif(mod2)
#Adjusted R-square to determine significant variables
library(leaps)
r2mod = regsubsets(adr ~ . - children - babies + adults:children + adults:babies,data = Resort)
r2s = summary(r2mod)
r2s$which
msize = 2:9
r2s$which[which.max(r2s$adjr2),]
selectVar = colnames(r2s$which)[r2s$which[which.max(r2s$adjr2),]]
selectVar = selectVar[-1]
selectVar
#Mallow's CP to determine significant variables
r2s$which[which.min(r2s$cp),]
selectVar = colnames(r2s$which)[r2s$which[which.min(r2s$cp),]]
selectVar = selectVar[-1]
selectVar
#Split into train and validation data
set.seed(100)
train = sample(1:nrow(Resort), 400)
#Starting Model
mlMod = randomForest(adr ~ . - children - babies + adults:children + adults:babies, data = Resort, subset = train)
mlMod
#plotting error vs number of trees
plot(mlMod)
oob.err=double(10)
test.err=double(10)
#mtry is the number of variables randomly chosen at each split
for(mtry in 1:10)
{
rf = randomForest(adr ~ . - children - babies + adults:children + adults:babies, data = Resort, subset = train, mtry = mtry,ntree = 400)
oob.err[mtry] = rf$mse[400] #Error of all Trees fitted
pred = predict(rf,Resort[-train,]) #Predictions on Test Set for each Tree
test.err[mtry] = with(Resort[-train,], mean((adr - pred)^2)) #Mean Squared Test Error
}
test.err
#minmize error at mtry = 4
min(test.err)
mlmod2 = randomForest(adr ~ . - children - babies + adults:children + adults:babies, data = Resort, subset = train, mtry = 4,ntree = 400)
predict(mlmod2,Resort[-train,])
View(Resort)
mlMod2 = randomForest(adr ~ . - children - babies + adults:children + adults:babies, data = Resort, subset = train, mtry = 4,ntree = 400)
mlMod2
#minmize error at mtry = 4
min(test.err)
